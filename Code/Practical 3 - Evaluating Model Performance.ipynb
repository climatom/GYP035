{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model performance \n",
    "## Overview\n",
    "Todaye are going to calculate error metrics to evaluate the skill of modules. By the end of this session we will have computed the:\n",
    "\n",
    "- Mean Error - $\\it{ME}$\n",
    "- Mean Absolute Error - $\\it{MAE}$\n",
    "- Skill Score - $\\it{SS}$\n",
    "\n",
    "These have been introduced to you in class so they are (hopefully) at least somewhat familiar to you by now. Briefly, the $\\it{ME}$ computes the means of the differences between the modelled ($\\it{m}$) and observed ($\\it{o}$) series: \n",
    "\n",
    "\\begin{equation*}\n",
    "ME=\\frac{1}{n}\\sum_{i=1}^n m_i - o_i\n",
    "\\end{equation*} \n",
    "\n",
    "whereas the $\\it{MAE}$ evaluates the mean of the $\\it{absolute}$ differences between the modelled and observed series:\n",
    "\n",
    "\\begin{equation*}\n",
    "MAE=\\frac{1}{n}\\sum_{i=1}^n | m_i - o_i |\n",
    "\\end{equation*} \n",
    "\n",
    "$\\it{SS}$ then provides us with a means to weigh the performance of competing models against one another:\n",
    "\n",
    "\\begin{equation*}\n",
    "SS=1-\\frac{E}{E_{ref}}\n",
    "\\end{equation*} \n",
    "\n",
    "Where $E_{ref}$ is the reference model against which we are evaluating performance. Clearly, as the error of our new model ($E$) tends to zero, SS approaches one. If the new model does not do such a good job as the reference ($E$ >$E_{ref}$) , the SS will be negative. \n",
    "\n",
    "In what follow we will import the data processed in the last session, where we modelled the day-of-year mean temperature climatology using two approaches:\n",
    "\n",
    "- 'doy_mean' = the temperature for any day was modelled as the arithmetic mean of all temperatures observed on the same calendar day (irrespective of year)\n",
    "- 'clim' = a sine wave was fitted to the daily mean temperature series\n",
    "\n",
    "We then will compute the error metrics on these models of the climatology, enabling us to evaluate which is a more appropriate climatology baseline in our assessment of daily mean air temperature forecasting skill for Loughborough campus. \n",
    "\n",
    "### Tips for the assignment \n",
    "Remember that your assignment is a write-up of the different modelling approaches, including your judegment of which method we should use in an operational forecasting application. All of these practical sessions will therfore be highly relevant, as they will essentially provide you with the output needed to complete the assignment. Note, however, that they will *not* repeat things -- even though that may be what you need to do. For example, today we will run code segments to compute error metrics, and it is expected that you will be able to do this $\\it{yourself}$ to evaluate the performance of other models we test in the future. What this means in practice is that you are expected to be able to copy/paste/edit the code segments I provide to you -- so that you can, for example, use the error metrics -- across *all* models -- to underpin your conclusion as to which model we should use at Loughborough University. I will regularly remind you of this in the closing instructions for each practical session (under the section: 'Your challenge -- to be completed before the next practical'). The challenges there will often require you to adapt code from other practical sessions to get the job done; and the output may be very useful for your assignment. \n",
    "\n",
    "## Instructions\n",
    "\n",
    "Begin by importing the pandas, numpy, and matplolib modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to read in the .csv file we saved at the end of the last session. To enable this code block to run, you will need to replace the string assigned to 'fin' with the full path of where you saved the file; if you are unsure where this is, check your code from the last session. \n",
    "\n",
    "Importantly, I want you to pay close attention to how we read in the file using the read_csv method of the pandas module. You will use this in almost every session, and you may need to recycle this code independently for your assignment. Note that there are three arguments (or, 'options') whose value I set: 'filepath_or_buffer', 'parse_dates', and 'index_col'. There are many more you *could* set (see here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), but this is all we need to make sure things are set up correctly. Often, getting your data read in correctly can take a few attempts as you iterate towards success: try -> inspect data -> change options -> try...\n",
    "\n",
    "Run the code below to import the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin=\"C:/Users/gytm3/OneDrive - Loughborough University/Teaching/GYP035/201920/DoY_climatology_tmean.csv\"\n",
    "data=pd.read_csv(filepath_or_buffer=fin,parse_dates=True,index_col=0) # Note: you could ignore the \"filepath_or_buffer=\" bit\n",
    "# and instead just have pd.read_csv(fin,parse_dates=True,index_col=0). Pandas will in this case just assume that the first \n",
    "# argument is the file path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you know that the data have been read in correctly? If you recall last session we touched on this with use of the command 'data.columns' -- which shows the columns attribute of data (the names of our variables -- the column headers). There is, however, an even better way of getting an idea of whether your data have been read in correctly: the 'head' method. Run the code below to use this to inspect data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>doy_mean</th>\n",
       "      <th>clim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-04-08</th>\n",
       "      <td>9.283875</td>\n",
       "      <td>10.221156</td>\n",
       "      <td>9.473328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-09</th>\n",
       "      <td>7.281344</td>\n",
       "      <td>7.598104</td>\n",
       "      <td>9.581643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-10</th>\n",
       "      <td>7.002740</td>\n",
       "      <td>7.250464</td>\n",
       "      <td>9.690412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-11</th>\n",
       "      <td>10.468208</td>\n",
       "      <td>7.798180</td>\n",
       "      <td>9.799605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-12</th>\n",
       "      <td>10.058594</td>\n",
       "      <td>7.817630</td>\n",
       "      <td>9.909188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-13</th>\n",
       "      <td>10.684615</td>\n",
       "      <td>9.476432</td>\n",
       "      <td>10.019128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-14</th>\n",
       "      <td>9.619125</td>\n",
       "      <td>9.503677</td>\n",
       "      <td>10.129395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-15</th>\n",
       "      <td>6.991844</td>\n",
       "      <td>9.448984</td>\n",
       "      <td>10.239954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-16</th>\n",
       "      <td>4.890479</td>\n",
       "      <td>9.318544</td>\n",
       "      <td>10.350773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-17</th>\n",
       "      <td>6.352000</td>\n",
       "      <td>10.925599</td>\n",
       "      <td>10.461819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  obs   doy_mean       clim\n",
       "TIMESTAMP                                  \n",
       "2016-04-08   9.283875  10.221156   9.473328\n",
       "2016-04-09   7.281344   7.598104   9.581643\n",
       "2016-04-10   7.002740   7.250464   9.690412\n",
       "2016-04-11  10.468208   7.798180   9.799605\n",
       "2016-04-12  10.058594   7.817630   9.909188\n",
       "2016-04-13  10.684615   9.476432  10.019128\n",
       "2016-04-14   9.619125   9.503677  10.129395\n",
       "2016-04-15   6.991844   9.448984  10.239954\n",
       "2016-04-16   4.890479   9.318544  10.350773\n",
       "2016-04-17   6.352000  10.925599  10.461819"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows=10 # Change this to see more or less rows from the 'head' method, below. \n",
    "data.head(nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is simple, and, hopefully, intuitive. \n",
    "\n",
    "These inspections are particularly important for a few reasons: 1) it reminds us how to 'index' variables within data (i.e. we can see what they are called); 2) it allows us to check that the date has been read in correctly. Pandas is super-smart when it comes to recognizing dates and interpreting them correctly... But it does make some mistakes, and this check is criticial. \n",
    "\n",
    "We are next going to compute the error metrics for doy_mean. Run the code below to find out the $\\it{ME}$ and $\\it{MAE}$. Read the comments to make sure you understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error is:  3.934967682215745e-17\n",
      "Mean absolute error is:  1.9636835620033317\n"
     ]
    }
   ],
   "source": [
    "me=np.mean(data[\"doy_mean\"]-data[\"obs\"]) # We use the 'mean' method of the numpy module to \n",
    "# calculate the mean of doy_mean-obs\n",
    "print(\"Mean error is: \", me) # normal use of the print function to show\n",
    "# the answer!\n",
    "mae=np.mean(np.abs(data[\"doy_mean\"]-data[\"obs\"])) # same as above, but here we use the \n",
    "# numpy method 'abs' to compute the absolute value of obs-doy_mean. Remember\n",
    "# that this means all negatives become positive, and all postives stay\n",
    "# positive. \n",
    "print(\"Mean absolute error is: \", mae) # normal use of the print function to show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed, a few questions that you should have answered by a member of staff:\n",
    "\n",
    "- Describe the result for $\\it{ME}$ in your own words (I want to see if you understand the number format)\n",
    "\n",
    "- Is $\\it{ME}$ a useful measure in this instance? If not, why?\n",
    "\n",
    "- What are the $\\it{units}$ for these error metrics? \n",
    "\n",
    "We are now going to repeat the computation for these error metrics, but this time using our own $\\it{function}$ to wrap up all the code for us. We do this because we $\\it{know}$ that we will be re-using the above code in the future and, as a rule, code that will be re-used frequently is best written in to a function -- it eliminates the chance of us making a mistake somewhere with copy/pasting: so long as we write the function correctly, it will $\\it{always}$ return the correct output if given the correct input. \n",
    "\n",
    "In the code section below, I show you another feature of syntax in python: everthing I write between \"\"\" is interpreted as comments not as code. This is a convenient way of writing extended notes to document how the function works, and any other important information about the function that the programmer thinks is relevant.\n",
    "\n",
    "Run the code below to have the python interpreter register our \"error_metrics\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metrics(obs,mod,summary=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the mean error (me) and \n",
    "    mean absolute (mae). Details of required input,\n",
    "    output, and notes are provided below. \n",
    "    \n",
    "    Input: \n",
    "    \n",
    "        - obs: column of a pandas dataframe (a Series type)\n",
    "               corresponding to the observed quanity\n",
    "        - mod: column of pandas dataframe (a Series type)\n",
    "               corresponding to the modelled quantity. \n",
    "        - summary: boolean ('True' or 'False'). If True\n",
    "               the function will print the me and mae. \n",
    "               \n",
    "    Output: \n",
    "    \n",
    "        - me:  mean error\n",
    "        - mae: mean absolute error\n",
    "        \n",
    "    Notes: \n",
    "        - No checking of input is performed. \n",
    "        - Requires numpy\n",
    "    \n",
    "    Change log:\n",
    "        - created 03/11/2019 by t.matthews@lboro.ac.uk\n",
    "        \n",
    "    \"\"\"\n",
    "    me=np.mean(mod-obs) \n",
    "    mae=np.mean(np.abs(mod-obs))\n",
    "    if summary:\n",
    "        # Note I use a differnt format \n",
    "        # to print here. It allows me \n",
    "        # to control how many digits\n",
    "        # behind the decimal point I \n",
    "        # show (here %.2f = 2)\n",
    "        print(\"ME = %.2f\"%me)\n",
    "        print(\"MAE = %.2f\"%mae)\n",
    "        \n",
    "    return me,mae # This 'returns' variables from 'inside' \n",
    "                  # the function to the 'outside' of the \n",
    "                  # function. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will look like nothing happens when you run the above, but python will add this function to its memory bank (for the duration of this session). \n",
    "\n",
    "Below we will test that the function works by using it to compute the errors for doy_mean; they should match the values returned earlier. To begin with, set summary to True. This will have the $\\it{function}$ print the error metrics to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ME = 0.00\n",
      "MAE = 1.96\n"
     ]
    }
   ],
   "source": [
    "me,mae=error_metrics(data[\"obs\"],data[\"doy_mean\"],summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll set summary to False and re-run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "me,mae=error_metrics(data[\"obs\"],data[\"clim\"],summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stops the function printing the results. We will, instead, print me and mae ourselves. We do this to demonstrate that the syntax me,mae=error_metrics(data[\"obs\"],data[\"doy_mean\"],summary=True) assigns the mean error computed by the function to the variable 'me', and the mean absolute error to the variable 'mae'; they now exist $\\it{outside}$ the function and can be used for any other analysis we want to conduct (in this case just printing their values): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ME = -0.00\n",
      "MAE = 2.26\n"
     ]
    }
   ],
   "source": [
    "print(\"ME = %.2f\"%me)\n",
    "print(\"MAE = %.2f\"%mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the variable 'summary' does not exist outside the function (because we didn't ask the function to $\\it{return}$ it). The following code will therefore return an error:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-7555db655df6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your challenge -- to be completed before the next practical\n",
    "- (1) Use the function we used above to compute the me and mae for clim (the sine wave model created last time).\n",
    "- (2) Compute $\\it{SS}$ using the $\\it{MAE}$. What does this tell us about the relative performance of the two  approaches for estimating the climatology?\n",
    "\n",
    "Tip: the arithmetic operations in python syntax are quite intuitive (e.g. '-' is the subtraction operator; and \"/\" is divide). If you ever need to look up this syntax, consult the docs: e.g. \"Mapping Operators to Functions\" table in https://docs.python.org/3/library/operator.html)\n",
    "- (3) Try writing your own function to compute the $\\it{SS}$. The answer you get from this function should match that computed in (2)\n",
    "\n",
    "Before trying (4) you need to download \"TomModel.csv\" from Learn. The data here are generated with exactly the same method (and have the same labels: doy_mean & clim), but they were calibrated using data from 2016-2018 and we have here 'predictions' for 2019 only. This means we will evaluate the models using data that were $\\it{not}$ involved at all in the fitting procedure. \n",
    "\n",
    "- (4) Compute the $\\it{MAE}$ for doy_mean and clim in TomModel.csv. Which suffers more in performance? Why do you think this is? [Hint: look back at how doy_mean is defined -- and ask yourself what would be the impact of removing an entire year's worth of data?]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
